{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatiron Capstone Project: Automated Breast Cancer Metastasis Detection\n",
    "### Convolutional Neural Network Development\n",
    "#### Marissa Eppes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUMMARY:** This notebook walks through the thought process and steps to tune and train a convolutional neural network (CNN) image classifier. Tiles are 256 x 256 pixel regions within lymph node biopsy whole slide images (WSI). Each individual tile is classified as either Cancer or Non-Cancer (often referred to as \"normal\") for the purpose of training and testing the model.\n",
    "\n",
    "All tiles for training and testing have already been extracted using train_tiles.py and test_tiles.py modules. Tiles are prepared for integration with the CNN using the cnn_prep.py module.\n",
    "\n",
    "The general strategy for CNN tuning/training is summarized as follows:\n",
    "* Train from scratch (transfer learning has been tried and did not appear promising for this specific application)\n",
    "* Start with relatively shallow model, smaller data sample, 10 - 20 epochs\n",
    "* Evaluate performance, Add/Subtract/Change 1 layer at a time,  Repeat\n",
    "* Scale up best-performing model(s), more data, 30 - 50+ epochs\n",
    "* Introduce image augmentation\n",
    "\n",
    "**Figure 1** portrays a diagram summarizing the development and selection process. Parameters for each of the models are shown in below code. \n",
    "\n",
    "**NOTE:** This notebook was executed using a GPU-accelerated AWS Sagemaker instance with 61GB RAM. There are several cases where model output had to be cleared to conserve RAM and continue modeling. Therefore, this notebook will not show all outputs, but will still attempt to demonstrate the model tuning process.\n",
    "\n",
    "<img src=\"modeling_diagram.png\">\n",
    "\n",
    "<center> Figure 1 <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "import cnn_prep as prep\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model, Sequential, Model\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers, models, layers\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k\n",
    "from keras import applications\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates lists of paths to all extracted cancer tiles and all extracted normal tiles\n",
    "\n",
    "cancer_train_glob = glob.glob(\n",
    "    \"/home/ec2-user/SageMaker/data/train/cancer/*.jpeg\")\n",
    "normal_train_glob = glob.glob(\n",
    "    \"/home/ec2-user/SageMaker/data/train/normal/*.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary CNN Development\n",
    "Several custom CNNs built from scratch will be tried with a 20,000-sample subset of data using 10 epochs. Best-performing CNN architectures will subsequently be scaled up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10,000 tiles from each class will be included for preliminary development (20,000 total)\n",
    "\n",
    "scale_down_number = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and prepares 10,000 tiles from each class chosen at random\n",
    "\n",
    "train_cancer = prep.cancer_train_jpegs_to_arrays(\n",
    "    cancer_train_glob, scale_down=scale_down_number)\n",
    "train_normal = prep.normal_train_jpegs_to_arrays(\n",
    "    normal_train_glob, scale_down=scale_down_number)\n",
    "train_data = np.concatenate((train_cancer, train_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that cancer data and normal data have been concatenated properly.\n",
    "# All outputs should be True.\n",
    "\n",
    "print((train_cancer[0] == train_data[0]).mean() == 1)\n",
    "print((train_cancer[-1] == train_data[scale_down_number-1]).mean() == 1)\n",
    "print((train_normal[-1] == train_data[-1]).mean() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del train_cancer\n",
    "del train_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns labels of 1 to cancer tiles and 0 to normal tiles\n",
    "\n",
    "train_labels = np.zeros(len(train_data))\n",
    "train_labels[0:scale_down_number] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that labels have been assigned in the proper order.\n",
    "# All outputs should be True.\n",
    "\n",
    "print(train_labels[0] == 1)\n",
    "print(train_labels[scale_down_number] == 0)\n",
    "print(train_labels[-1] == 0)\n",
    "print(train_labels.mean() == 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs an 80/20 train/test split to create a validation set.\n",
    "# Original train_data is deleted to conserve RAM.\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data, train_labels, test_size=0.20, random_state=27)\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets desired parameters for CNN training\n",
    "\n",
    "image_width, image_height = 256, 256\n",
    "batch_size = 64\n",
    "train_steps_per_epoch = int(len(X_train)/batch_size)\n",
    "val_steps_per_epoch = int(len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates ImageDataGenerator objects for training and validation sets\n",
    "# Normalizes RGB values in tiles\n",
    "\n",
    "train_IDG = ImageDataGenerator(rescale=1./255)\n",
    "val_IDG = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_IDG.flow(X_train, y_train, batch_size=batch_size)\n",
    "val_generator = val_IDG.flow(X_val, y_val, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model to try\n",
    "\n",
    "cnn1 = models.Sequential()\n",
    "cnn1.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(\n",
    "    image_width, image_height,  3), padding='SAME'))\n",
    "cnn1.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn1.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn1.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn1.add(layers.Flatten())\n",
    "cnn1.add(layers.Dense(32, activation='relu'))\n",
    "cnn1.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn1.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets callback parameters\n",
    "\n",
    "saving_weights_1 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights1.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=10)\n",
    "\n",
    "reduce_lr_1 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_1 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_1 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_1 = keras.callbacks.CSVLogger('training_1.log')\n",
    "print(cnn1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit first 10 epochs\n",
    "\n",
    "cnn1.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_1, early_stop_1, nan_problem_1, reduce_lr_1, saving_weights_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1.save('cnn1_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an additional 10 epochs\n",
    "\n",
    "cnn1.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_1, early_stop_1, nan_problem_1, reduce_lr_1, saving_weights_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1.save('cnn1_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2: Change optimizer from SGD to ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = models.Sequential()\n",
    "cnn2.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(\n",
    "    image_width, image_height,  3), padding='SAME'))\n",
    "cnn2.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn2.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn2.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn2.add(layers.Flatten())\n",
    "cnn2.add(layers.Dense(32, activation='relu'))\n",
    "cnn2.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn2.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_2 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights2.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=10)\n",
    "\n",
    "reduce_lr_2 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_2 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_2 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_2 = keras.callbacks.CSVLogger('training_2.log')\n",
    "print(cnn2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_2, early_stop_2, nan_problem_2, reduce_lr_2, saving_weights_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2.save('cnn2_10.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #3: Change optimizer back to SGD, add another dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3 = models.Sequential()\n",
    "cnn3.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(\n",
    "    image_width, image_height,  3), padding='SAME'))\n",
    "cnn3.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn3.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn3.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn3.add(layers.Flatten())\n",
    "cnn3.add(layers.Dense(32, activation='relu'))\n",
    "cnn3.add(layers.Dense(32, activation='relu'))\n",
    "cnn3.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn3.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_3 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights3.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=10)\n",
    "\n",
    "reduce_lr_3 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_3 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_3 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_3 = keras.callbacks.CSVLogger('training_3.log')\n",
    "print(cnn3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_3, early_stop_3, nan_problem_3, reduce_lr_3, saving_weights_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3.save('cnn3_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an additional 10 epochs\n",
    "\n",
    "cnn3.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_3, early_stop_3, nan_problem_3, reduce_lr_3, saving_weights_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3.save('cnn3_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #4: Take away added dense layer, add convolution/pool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn4 = models.Sequential()\n",
    "cnn4.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(\n",
    "    image_width, image_height,  3), padding='SAME'))\n",
    "cnn4.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn4.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(\n",
    "    image_width, image_height,  3), padding='SAME'))\n",
    "cnn4.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn4.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn4.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn4.add(layers.Flatten())\n",
    "cnn4.add(layers.Dense(32, activation='relu'))\n",
    "cnn4.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn4.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_4 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights4.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=10)\n",
    "\n",
    "reduce_lr_4 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_4 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_4 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_4 = keras.callbacks.CSVLogger('training_4.log')\n",
    "print(cnn4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn4.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_4, early_stop_4, nan_problem_4, reduce_lr_4, saving_weights_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn4.save('cnn4_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an additional 10 epochs\n",
    "\n",
    "cnn4.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_4, early_stop_4, nan_problem_4, reduce_lr_4, saving_weights_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn4.save('cnn4_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #5: Keep added convolution layer, but increase filter size of first convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn5 = models.Sequential()\n",
    "cnn5.add(layers.Conv2D(128, (3, 3), activation='relu',\n",
    "                       input_shape=(image_width, image_height,  3), padding='SAME'))\n",
    "cnn5.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn5.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(\n",
    "    image_width, image_height,  3), padding='SAME'))\n",
    "cnn5.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn5.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn5.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn5.add(layers.Flatten())\n",
    "cnn5.add(layers.Dense(32, activation='relu'))\n",
    "cnn5.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn5.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_5 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights5.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=10)\n",
    "\n",
    "reduce_lr_5 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_5 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_5 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_5 = keras.callbacks.CSVLogger('training_5.log')\n",
    "print(cnn5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn5.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_5, early_stop_5, nan_problem_5, reduce_lr_5, saving_weights_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn5.save('cnn5_10.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #6: Take first convolution filter back down to 64, lower second convolution filter to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6 = models.Sequential()\n",
    "cnn6.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(\n",
    "    image_width, image_height,  3), padding='SAME'))\n",
    "cnn6.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(\n",
    "    image_width, image_height,  3), padding='SAME'))\n",
    "cnn6.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn6.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6.add(layers.Flatten())\n",
    "cnn6.add(layers.Dense(32, activation='relu'))\n",
    "cnn6.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn6.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights6.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=10)\n",
    "\n",
    "reduce_lr_6 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6 = keras.callbacks.CSVLogger('training_6.log')\n",
    "print(cnn6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_6, early_stop_6, nan_problem_6, reduce_lr_6, saving_weights_6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6.save('cnn6_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an additional 10 epochs\n",
    "\n",
    "cnn6.fit_generator(train_generator, epochs=10, steps_per_epoch=train_steps_per_epoch, validation_data=val_generator,\n",
    "                   validation_steps=val_steps_per_epoch, callbacks=[csv_logger_6, early_stop_6, nan_problem_6, reduce_lr_6, saving_weights_6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6.save('cnn6_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two scaled-up models will be run according to architecture from Models 1 and 6. \n",
    "## 60,000 samples will be used for training/validation, and 30 - 60 epochs will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and prepares 30,000 tiles from each class chosen at random\n",
    "\n",
    "train_cancer_30k = prep.cancer_train_jpegs_to_arrays(\n",
    "    cancer_train_glob, scale_down=30000)\n",
    "train_normal_30k = prep.normal_train_jpegs_to_arrays(\n",
    "    normal_train_glob, scale_down=30000)\n",
    "train_data_60k = np.concatenate((train_cancer_30k, train_normal_30k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that cancer data and normal data have been\n",
    "# concatenated properly. All outputs should be True.\n",
    "\n",
    "print((train_cancer_30k[0] == train_data_60k[0]).mean() == 1)\n",
    "print((train_cancer_30k[-1] ==\n",
    "       train_data_60k[len(train_data_60k/2)-1]).mean() == 1)\n",
    "print((train_normal_30k[-1] == train_data_60k[-1]).mean() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del train_cancer_30k\n",
    "del train_normal_30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns labels\n",
    "\n",
    "train_labels_60k = np.zeros(len(train_data_60k))\n",
    "train_labels_60k[0:int(len(train_labels_60k)/2)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that labels have been assigned in the proper order.\n",
    "# All outputs should be True.\n",
    "\n",
    "print(train_labels_60k[0] == 1)\n",
    "print(train_labels_60k[int(len(train_labels_60k)/2)] == 0)\n",
    "print(train_labels_60k[-1] == 0)\n",
    "print(train_labels_60k.mean() == 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_60k = to_categorical(train_labels_60k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs an 80/20 train/test split to create a validation set.\n",
    "\n",
    "X_train_60k, X_val_60k, y_train_60k, y_val_60k = train_test_split(\n",
    "    train_data_60k, train_labels_60k, test_size=0.20, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original train_data is deleted to conserve RAM.\n",
    "\n",
    "del train_data_60k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets desired parameters for CNN training\n",
    "\n",
    "image_width, image_height = 256, 256\n",
    "batch_size_60k = 64\n",
    "train_steps_per_epoch_60k = int(len(X_train_60k)/batch_size_60k)\n",
    "val_steps_per_epoch_60k = int(len(X_val_60k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates ImageDataGenerator objects for training and validation sets\n",
    "# Normalizes RGB values in tiles\n",
    "\n",
    "train_IDG_60k = ImageDataGenerator(rescale=1./255)\n",
    "val_IDG_60k = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies ImageDataGenerator\n",
    "\n",
    "train_generator_60k = train_IDG_60k.flow(\n",
    "    X_train_60k, y_train_60k, batch_size=batch_size_60k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del X_train_60k\n",
    "del y_train_60k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies ImageDataGenerator\n",
    "\n",
    "val_generator_60k = val_IDG_60k.flow(X_val_60k, y_val_60k, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del X_val_60k\n",
    "del y_val_60k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled-Up Model #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k = models.Sequential()\n",
    "cnn1_60k.add(layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                           input_shape=(image_width, image_height,  3), padding='SAME'))\n",
    "cnn1_60k.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn1_60k.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn1_60k.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn1_60k.add(layers.Flatten())\n",
    "cnn1_60k.add(layers.Dense(32, activation='relu'))\n",
    "cnn1_60k.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn1_60k.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 1 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_1_60k_1 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights1_60k_1.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_1_60k_1 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_1_60k_1 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_1_60k_1 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_1_60k_1 = keras.callbacks.CSVLogger('training_1_60k_1.log')\n",
    "print(cnn1_60k.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.fit_generator(train_generator_60k, epochs=10, steps_per_epoch=train_steps_per_epoch_60k, validation_data=val_generator_60k,\n",
    "                       validation_steps=val_steps_per_epoch_60k, callbacks=[csv_logger_1_60k_1, early_stop_1_60k_1, nan_problem_1_60k_1, reduce_lr_1_60k_1, saving_weights_1_60k_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.save('cnn1_60k_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 11 - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_1_60k_2 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights1_60k_2.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_1_60k_2 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_1_60k_2 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_1_60k_2 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_1_60k_2 = keras.callbacks.CSVLogger('training_1_60k_2.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.fit_generator(train_generator_60k, epochs=10, steps_per_epoch=train_steps_per_epoch_60k, validation_data=val_generator_60k,\n",
    "                       validation_steps=val_steps_per_epoch_60k, callbacks=[csv_logger_1_60k_2, early_stop_1_60k_2, nan_problem_1_60k_2, reduce_lr_1_60k_2, saving_weights_1_60k_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.save('cnn1_60k_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 21 - 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_1_60k_3 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights1_60k_3.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_1_60k_3 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_1_60k_3 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_1_60k_3 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_1_60k_3 = keras.callbacks.CSVLogger('training_1_60k_3.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.fit_generator(train_generator_60k, epochs=10, steps_per_epoch=train_steps_per_epoch_60k, validation_data=val_generator_60k,\n",
    "                       validation_steps=val_steps_per_epoch_60k, callbacks=[csv_logger_1_60k_3, early_stop_1_60k_3, nan_problem_1_60k_3, reduce_lr_1_60k_3, saving_weights_1_60k_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.save('cnn1_60k_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled-Up Model #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k = models.Sequential()\n",
    "cnn6_60k.add(layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                           input_shape=(image_width, image_height,  3), padding='SAME'))\n",
    "cnn6_60k.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6_60k.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                           input_shape=(image_width, image_height,  3), padding='SAME'))\n",
    "cnn6_60k.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6_60k.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn6_60k.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6_60k.add(layers.Flatten())\n",
    "cnn6_60k.add(layers.Dense(32, activation='relu'))\n",
    "cnn6_60k.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn6_60k.compile(loss='binary_crossentropy', optimizer=\"sgd\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 1 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_60k_1 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights6_60k_1.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_60k_1 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_60k_1 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_60k_1 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_60k_1 = keras.callbacks.CSVLogger('training_6_60k_1.log')\n",
    "print(cnn6_60k.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.fit_generator(train_generator_60k, epochs=10, steps_per_epoch=train_steps_per_epoch_60k, validation_data=val_generator_60k,\n",
    "                       validation_steps=val_steps_per_epoch_60k, callbacks=[csv_logger_6_60k_1, early_stop_6_60k_1, nan_problem_6_60k_1, reduce_lr_6_60k_1, saving_weights_6_60k_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.save('cnn6_60k_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 11 - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_60k_2 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights6_60k_2.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_60k_2 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_60k_2 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_60k_2 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_60k_2 = keras.callbacks.CSVLogger('training_6_60k_2.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.fit_generator(train_generator_60k, epochs=10, steps_per_epoch=train_steps_per_epoch_60k, validation_data=val_generator_60k,\n",
    "                       validation_steps=val_steps_per_epoch_60k, callbacks=[csv_logger_6_60k_2, early_stop_6_60k_2, nan_problem_6_60k_2, reduce_lr_6_60k_2, saving_weights_6_60k_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.save('cnn6_60k_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 21 - 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_60k_3 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights6_60k_3.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_60k_3 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_60k_3 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_60k_3 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_60k_3 = keras.callbacks.CSVLogger('training_6_60k_3.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.fit_generator(train_generator_60k, epochs=10, steps_per_epoch=train_steps_per_epoch_60k, validation_data=val_generator_60k,\n",
    "                       validation_steps=val_steps_per_epoch_60k, callbacks=[csv_logger_6_60k_3, early_stop_6_60k_3, nan_problem_6_60k_3, reduce_lr_6_60k_3, saving_weights_6_60k_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.save('cnn6_60k_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 31 - 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_60k_4 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights6_60k_4.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_60k_4 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_60k_4 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_60k_4 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_60k_4 = keras.callbacks.CSVLogger('training_6_60k_4.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.fit_generator(train_generator_60k, epochs=10, steps_per_epoch=train_steps_per_epoch_60k, validation_data=val_generator_60k,\n",
    "                       validation_steps=val_steps_per_epoch_60k, callbacks=[csv_logger_6_60k_4, early_stop_6_60k_4, nan_problem_6_60k_4, reduce_lr_6_60k_4, saving_weights_6_60k_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.save('cnn6_60k_4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 41 - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_60k_5 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights6_60k_5.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_60k_5 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_60k_5 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_60k_5 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_60k_5 = keras.callbacks.CSVLogger('training_6_60k_5.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.fit_generator(train_generator_60k, epochs=10, steps_per_epoch=train_steps_per_epoch_60k, validation_data=val_generator_60k,\n",
    "                       validation_steps=val_steps_per_epoch_60k, callbacks=[csv_logger_6_60k_5, early_stop_6_60k_5, nan_problem_6_60k_5, reduce_lr_6_60k_5, saving_weights_6_60k_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_60k.save('cnn6_60k_5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take another random sample of data and implement Image Augmentation to continue training models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and prepares 30,000 tiles from each class chosen at random\n",
    "\n",
    "train_cancer_30k_aug = prep.cancer_train_jpegs_to_arrays(\n",
    "    cancer_train_glob, scale_down=30000)\n",
    "train_normal_30k_aug = prep.normal_train_jpegs_to_arrays(\n",
    "    normal_train_glob, scale_down=30000)\n",
    "train_data_60k_aug = np.concatenate(\n",
    "    (train_cancer_30k_aug, train_normal_30k_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that cancer data and normal data have been\n",
    "# concatenated properly. All outputs should be True.\n",
    "\n",
    "print((train_cancer_30k_aug[0] == train_data_60k_aug[0]).mean() == 1)\n",
    "print((train_cancer_30k_aug[-1] ==\n",
    "       train_data_60k_aug[len(train_data_60k_aug/2)-1]).mean() == 1)\n",
    "print((train_normal_30k_aug[-1] == train_data_60k_aug[-1]).mean() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del train_cancer_30k_aug\n",
    "del train_normal_30k_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns labels\n",
    "\n",
    "train_labels_60k_aug = np.zeros(len(train_data_60k_aug))\n",
    "train_labels_60k_aug[0:int(len(train_labels_60k_aug)/2)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that labels have been assigned in the proper order.\n",
    "# All outputs should be True.\n",
    "\n",
    "print(train_labels_60k_aug[0] == 1)\n",
    "print(train_labels_60k_aug[int(len(train_labels_60k_aug)/2)] == 0)\n",
    "print(train_labels_60k_aug[-1] == 0)\n",
    "print(train_labels_60k_aug.mean() == 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_60k_aug = to_categorical(train_labels_60k_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs an 80/20 train/test split to create a validation set.\n",
    "\n",
    "\n",
    "X_train_60k_aug, X_val_60k_aug, y_train_60k_aug, y_val_60k_aug = train_test_split(\n",
    "    train_data_60k_aug, train_labels_60k_aug, test_size=0.20, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original train_data is deleted to conserve RAM.\n",
    "\n",
    "del train_data_60k_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets desired parameters for CNN training\n",
    "\n",
    "image_width, image_height = 256, 256\n",
    "batch_size_60k_aug = 64\n",
    "train_steps_per_epoch_60k_aug = int(len(X_train_60k_aug)/batch_size_60k_aug)\n",
    "val_steps_per_epoch_60k_aug = int(len(X_val_60k_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates ImageDataGenerator objects for training and validation sets\n",
    "# Normalizes RGB values in tiles\n",
    "# Implements Image Augmentation - random flips and rotations\n",
    "\n",
    "train_IDG_60k_aug = ImageDataGenerator(\n",
    "    rescale=1./255, rotation_range=180, horizontal_flip=True, vertical_flip=True)\n",
    "val_IDG_60k_aug = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies ImageDataGenerator\n",
    "\n",
    "train_generator_60k_aug = train_IDG_60k_aug.flow(\n",
    "    X_train_60k_aug, y_train_60k_aug, batch_size=batch_size_60k_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del X_train_60k_aug\n",
    "del y_train_60k_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies ImageDataGenerator\n",
    "\n",
    "val_generator_60k_aug = val_IDG_60k_aug.flow(\n",
    "    X_val_60k_aug, y_val_60k_aug, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del X_val_60k_aug\n",
    "del y_val_60k_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled-Up Model #1 with New Data and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 31 - 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_1_60k_4_aug = keras.callbacks.ModelCheckpoint(\n",
    "    'weights1_60k_4_aug.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_1_60k_4_aug = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_1_60k_4_aug = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_1_60k_4_aug = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_1_60k_4_aug = keras.callbacks.CSVLogger('training_1_60k_4_aug.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.fit_generator(train_generator_60k_aug, epochs=10, steps_per_epoch=train_steps_per_epoch_60k_aug, validation_data=val_generator_60k_aug, validation_steps=val_steps_per_epoch_60k_aug, callbacks=[\n",
    "                       csv_logger_1_60k_4_aug, early_stop_1_60k_4_aug, nan_problem_1_60k_4_aug, reduce_lr_1_60k_4_aug, saving_weights_1_60k_4_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.save('cnn1_60k_4_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 41 - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_1_60k_5_aug = keras.callbacks.ModelCheckpoint(\n",
    "    'weights1_60k_5_aug.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_1_60k_5_aug = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_1_60k_5_aug = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_1_60k_5_aug = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_1_60k_5_aug = keras.callbacks.CSVLogger('training_1_60k_5_aug.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.fit_generator(train_generator_60k_aug, epochs=10, steps_per_epoch=train_steps_per_epoch_60k_aug, validation_data=val_generator_60k_aug, validation_steps=val_steps_per_epoch_60k_aug, callbacks=[\n",
    "                       csv_logger_1_60k_5_aug, early_stop_1_60k_5_aug, nan_problem_1_60k_5_aug, reduce_lr_1_60k_5_aug, saving_weights_1_60k_5_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.save('cnn1_60k_5_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 51 - 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_1_60k_6_aug = keras.callbacks.ModelCheckpoint(\n",
    "    'weights1_60k_6_aug.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=0, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_1_60k_6_aug = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=0, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_1_60k_6_aug = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_1_60k_6_aug = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_1_60k_6_aug = keras.callbacks.CSVLogger('training_1_60k_6_aug.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.fit_generator(train_generator_60k_aug, epochs=10, steps_per_epoch=train_steps_per_epoch_60k_aug, validation_data=val_generator_60k_aug, validation_steps=val_steps_per_epoch_60k_aug, callbacks=[\n",
    "                       csv_logger_1_60k_6_aug, early_stop_1_60k_6_aug, nan_problem_1_60k_6_aug, reduce_lr_1_60k_6_aug, saving_weights_1_60k_6_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1_60k.save('cnn1_60k_6_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model: \n",
    "## Scaled-Up Model # 6, Retrain with all New Data, Increase Size of Training and Validation Set, Add Seed, Implement Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and prepares 35,000 tiles from each class chosen at random\n",
    "# This is the most that this particular server would allow before\n",
    "# running out of memory\n",
    "\n",
    "train_cancer_6_final = prep.cancer_train_jpegs_to_arrays(\n",
    "    cancer_train_glob, scale_down=35000, seed=52)\n",
    "train_normal_6_final = prep.normal_train_jpegs_to_arrays(\n",
    "    normal_train_glob, scale_down=35000, seed=52)\n",
    "train_data_6_final = np.concatenate(\n",
    "    (train_cancer_6_final, train_normal_6_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that cancer data and normal data have been\n",
    "# concatenated properly. All outputs should be True.\n",
    "print((train_cancer_6_final[0] == train_data_6_final[0]).mean() == 1)\n",
    "print((train_cancer_6_final[-1] ==\n",
    "       train_data_6_final[len(train_data_6_final/2)-1]).mean() == 1)\n",
    "print((train_normal_6_final[-1] == train_data_6_final[-1]).mean() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del train_cancer_6_final\n",
    "del train_normal_6_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns labels\n",
    "\n",
    "train_labels_6_final = np.zeros(len(train_data_6_final))\n",
    "train_labels_6_final[0:int(len(train_labels_6_final)/2)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that labels have been assigned in the proper order.\n",
    "# All outputs should be True.\n",
    "\n",
    "print(train_labels_6_final[0] == 1)\n",
    "print(train_labels_6_final[int(len(train_labels_6_final)/2)] == 0)\n",
    "print(train_labels_6_final[-1] == 0)\n",
    "print(train_labels_6_final.mean() == 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_6_final = to_categorical(train_labels_6_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs an 75/25 train/test split to create a validation set.\n",
    "\n",
    "X_train_6_final, X_val_6_final, y_train_6_final, y_val_6_final = train_test_split(\n",
    "    train_data_6_final, train_labels_6_final, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del train_data_6_final\n",
    "del train_labels_6_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets desired parameters for CNN training\n",
    "\n",
    "image_width, image_height = 256, 256\n",
    "batch_size_6_final = 64\n",
    "train_steps_per_epoch_6_final = int(len(X_train_6_final)/batch_size_6_final)\n",
    "val_steps_per_epoch_6_final = int(len(X_val_6_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates ImageDataGenerator objects for training and validation sets\n",
    "# Normalizes RGB values in tiles\n",
    "\n",
    "train_IDG_6_final = ImageDataGenerator(rescale=1./255)\n",
    "val_IDG_6_final = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies ImageDataGenerator\n",
    "\n",
    "train_generator_6_final = train_IDG_6_final.flow(\n",
    "    X_train_6_final, y_train_6_final, batch_size=batch_size_6_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del X_train_6_final\n",
    "del y_train_6_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies ImageDataGenerator\n",
    "\n",
    "val_generator_6_final = val_IDG_6_final.flow(\n",
    "    X_val_6_final, y_val_6_final, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del X_val_6_final\n",
    "del y_val_6_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final = models.Sequential()\n",
    "cnn6_final.add(layers.Conv2D(64, (3, 3), activation='relu',\n",
    "                             input_shape=(image_width, image_height,  3), padding='SAME'))\n",
    "cnn6_final.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6_final.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                             input_shape=(image_width, image_height,  3), padding='SAME'))\n",
    "cnn6_final.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6_final.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "cnn6_final.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn6_final.add(layers.Flatten())\n",
    "cnn6_final.add(layers.Dense(32, activation='relu'))\n",
    "cnn6_final.add(layers.Dense(2, activation='sigmoid'))\n",
    "cnn6_final.compile(loss='binary_crossentropy',\n",
    "                   optimizer=\"sgd\", metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 1 - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_final_1 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights_6_final_1.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=2, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_final_1 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=2, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_final_1 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_final_1 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=2, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_final_1 = keras.callbacks.CSVLogger('training_6_final_1.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.fit_generator(train_generator_6_final, epochs=10, steps_per_epoch=train_steps_per_epoch_6_final, validation_data=val_generator_6_final,\n",
    "                         validation_steps=val_steps_per_epoch_6_final, callbacks=[csv_logger_6_final_1, early_stop_6_final_1, nan_problem_6_final_1, reduce_lr_6_final_1, saving_weights_6_final_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.save('cnn6_final_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 11 - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_final_2 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights_6_final_2.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=2, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_final_2 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=2, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_final_2 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_final_2 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=2, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_final_2 = keras.callbacks.CSVLogger('training_6_final_2.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.fit_generator(train_generator_6_final, epochs=10, steps_per_epoch=train_steps_per_epoch_6_final, validation_data=val_generator_6_final,\n",
    "                         validation_steps=val_steps_per_epoch_6_final, callbacks=[csv_logger_6_final_2, early_stop_6_final_2, nan_problem_6_final_2, reduce_lr_6_final_2, saving_weights_6_final_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.save('cnn6_final_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 21 -30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_final_3 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights_6_final_3.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=2, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_final_3 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=2, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_final_3 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_final_3 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=2, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_final_3 = keras.callbacks.CSVLogger('training_6_final_3.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.fit_generator(train_generator_6_final, epochs=10, steps_per_epoch=train_steps_per_epoch_6_final, validation_data=val_generator_6_final,\n",
    "                         validation_steps=val_steps_per_epoch_6_final, callbacks=[csv_logger_6_final_3, early_stop_6_final_3, nan_problem_6_final_3, reduce_lr_6_final_3, saving_weights_6_final_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.save('cnn6_final_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 31 -40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_final_4 = keras.callbacks.ModelCheckpoint(\n",
    "    'weights_6_final_4.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=2, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_final_4 = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=2, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_final_4 = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_final_4 = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=2, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_final_4 = keras.callbacks.CSVLogger('training_6_final_4.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.fit_generator(train_generator_6_final, epochs=10, steps_per_epoch=train_steps_per_epoch_6_final, validation_data=val_generator_6_final,\n",
    "                         validation_steps=val_steps_per_epoch_6_final, callbacks=[csv_logger_6_final_4, early_stop_6_final_4, nan_problem_6_final_4, reduce_lr_6_final_4, saving_weights_6_final_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.save('cnn6_final_4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final = load_model('cnn6_final_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Seed:  52\n",
      "Using Seed:  52\n"
     ]
    }
   ],
   "source": [
    "# Reload same training data as before\n",
    "\n",
    "train_cancer_6_final = prep.cancer_train_jpegs_to_arrays(\n",
    "    cancer_train_glob, scale_down=35000, seed=52)\n",
    "train_normal_6_final = prep.normal_train_jpegs_to_arrays(\n",
    "    normal_train_glob, scale_down=35000, seed=52)\n",
    "train_data_6_final = np.concatenate(\n",
    "    (train_cancer_6_final, train_normal_6_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code confirms that cancer data and normal data have been\n",
    "# concatenated properly. All outputs should be True.\n",
    "\n",
    "print((train_cancer_6_final[0] == train_data_6_final[0]).mean() == 1)\n",
    "print((train_cancer_6_final[-1] ==\n",
    "       train_data_6_final[len(train_data_6_final/2)-1]).mean() == 1)\n",
    "print((train_normal_6_final[-1] == train_data_6_final[-1]).mean() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del train_cancer_6_final\n",
    "del train_normal_6_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigns labels\n",
    "\n",
    "train_labels_6_final = np.zeros(len(train_data_6_final))\n",
    "train_labels_6_final[0:int(len(train_labels_6_final)/2)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Below code confirms that labels have been assigned in the proper order.\n",
    "# All outputs should be True.\n",
    "\n",
    "print(train_labels_6_final[0] == 1)\n",
    "print(train_labels_6_final[int(len(train_labels_6_final)/2)] == 0)\n",
    "print(train_labels_6_final[-1] == 0)\n",
    "print(train_labels_6_final.mean() == 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_6_final = to_categorical(train_labels_6_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs an 75/25 train/test split to create a validation set.\n",
    "\n",
    "X_train_6_final, X_val_6_final, y_train_6_final, y_val_6_final = train_test_split(\n",
    "    train_data_6_final, train_labels_6_final, test_size=0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del train_data_6_final\n",
    "del train_labels_6_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets desired parameters for CNN training\n",
    "\n",
    "image_width, image_height = 256, 256\n",
    "batch_size_6_final = 64\n",
    "train_steps_per_epoch_6_final = int(len(X_train_6_final)/batch_size_6_final)\n",
    "val_steps_per_epoch_6_final = int(len(X_val_6_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates ImageDataGenerator objects for training and validation sets\n",
    "# Normalizes RGB values in tiles\n",
    "# Implements Image Augmentation - random flips and rotations\n",
    "\n",
    "train_IDG_6_final = ImageDataGenerator(\n",
    "    rescale=1./255, rotation_range=180, horizontal_flip=True, vertical_flip=True)\n",
    "val_IDG_6_final = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_6_final = train_IDG_6_final.flow(\n",
    "    X_train_6_final, y_train_6_final, batch_size=batch_size_6_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del X_train_6_final\n",
    "del y_train_6_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator_6_final = val_IDG_6_final.flow(\n",
    "    X_val_6_final, y_val_6_final, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conserves RAM\n",
    "\n",
    "del X_val_6_final\n",
    "del y_val_6_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 41 -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_final_5_aug = keras.callbacks.ModelCheckpoint(\n",
    "    'weights_6_final_5_aug.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=2, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_final_5_aug = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=2, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_final_5_aug = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_final_5_aug = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=2, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_final_5_aug = keras.callbacks.CSVLogger(\n",
    "    'training_6_final_5_aug.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "820/820 [==============================] - 871s 1s/step - loss: 0.2139 - acc: 0.9197 - val_loss: 0.1755 - val_acc: 0.9330\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17549, saving model to weights_6_final_5_aug.01-0.18.hdf5\n",
      "Epoch 2/10\n",
      "820/820 [==============================] - 864s 1s/step - loss: 0.1934 - acc: 0.9272 - val_loss: 0.1904 - val_acc: 0.9290\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.17549\n",
      "Epoch 3/10\n",
      "820/820 [==============================] - 862s 1s/step - loss: 0.1873 - acc: 0.9299 - val_loss: 0.2131 - val_acc: 0.9192\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.17549\n",
      "Epoch 4/10\n",
      "820/820 [==============================] - 883s 1s/step - loss: 0.1864 - acc: 0.9315 - val_loss: 0.1848 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.17549\n",
      "Epoch 5/10\n",
      "820/820 [==============================] - 862s 1s/step - loss: 0.1801 - acc: 0.9331 - val_loss: 0.1698 - val_acc: 0.9374\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.17549 to 0.16982, saving model to weights_6_final_5_aug.05-0.17.hdf5\n",
      "Epoch 6/10\n",
      "820/820 [==============================] - 858s 1s/step - loss: 0.1778 - acc: 0.9349 - val_loss: 0.1831 - val_acc: 0.9302\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.16982\n",
      "Epoch 7/10\n",
      "820/820 [==============================] - 873s 1s/step - loss: 0.1717 - acc: 0.9360 - val_loss: 0.1522 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.16982 to 0.15218, saving model to weights_6_final_5_aug.07-0.15.hdf5\n",
      "Epoch 8/10\n",
      "820/820 [==============================] - 883s 1s/step - loss: 0.1693 - acc: 0.9379 - val_loss: 0.1464 - val_acc: 0.9455\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.15218 to 0.14642, saving model to weights_6_final_5_aug.08-0.15.hdf5\n",
      "Epoch 9/10\n",
      "820/820 [==============================] - 884s 1s/step - loss: 0.1698 - acc: 0.9358 - val_loss: 0.1794 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14642\n",
      "Epoch 10/10\n",
      "820/820 [==============================] - 864s 1s/step - loss: 0.1678 - acc: 0.9376 - val_loss: 0.1501 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14642\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fe725b0b8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn6_final.fit_generator(train_generator_6_final, epochs=10, steps_per_epoch=train_steps_per_epoch_6_final, validation_data=val_generator_6_final, validation_steps=val_steps_per_epoch_6_final, callbacks=[\n",
    "                         csv_logger_6_final_5_aug, early_stop_6_final_5_aug, nan_problem_6_final_5_aug, reduce_lr_6_final_5_aug, saving_weights_6_final_5_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.save('cnn6_final_5_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 51 - 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_final_6_aug = keras.callbacks.ModelCheckpoint(\n",
    "    'weights_6_final_6_aug.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=2, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_final_6_aug = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=2, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_final_6_aug = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_final_6_aug = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=2, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_final_6_aug = keras.callbacks.CSVLogger(\n",
    "    'training_6_final_6_aug.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "820/820 [==============================] - 866s 1s/step - loss: 0.1646 - acc: 0.9398 - val_loss: 0.1476 - val_acc: 0.9463\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14761, saving model to weights_6_final_6_aug.01-0.15.hdf5\n",
      "Epoch 2/10\n",
      "820/820 [==============================] - 859s 1s/step - loss: 0.1626 - acc: 0.9395 - val_loss: 0.1439 - val_acc: 0.9460\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14761 to 0.14392, saving model to weights_6_final_6_aug.02-0.14.hdf5\n",
      "Epoch 3/10\n",
      "820/820 [==============================] - 873s 1s/step - loss: 0.1604 - acc: 0.9407 - val_loss: 0.1710 - val_acc: 0.9384\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.14392\n",
      "Epoch 4/10\n",
      "820/820 [==============================] - 858s 1s/step - loss: 0.1605 - acc: 0.9403 - val_loss: 0.1535 - val_acc: 0.9430\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.14392\n",
      "Epoch 5/10\n",
      "820/820 [==============================] - 824s 1s/step - loss: 0.1583 - acc: 0.9411 - val_loss: 0.1418 - val_acc: 0.9476\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.14392 to 0.14183, saving model to weights_6_final_6_aug.05-0.14.hdf5\n",
      "Epoch 6/10\n",
      "820/820 [==============================] - 828s 1s/step - loss: 0.1576 - acc: 0.9414 - val_loss: 0.1378 - val_acc: 0.9488\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.14183 to 0.13783, saving model to weights_6_final_6_aug.06-0.14.hdf5\n",
      "Epoch 7/10\n",
      "820/820 [==============================] - 829s 1s/step - loss: 0.1551 - acc: 0.9431 - val_loss: 0.1391 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.13783\n",
      "Epoch 8/10\n",
      "820/820 [==============================] - 832s 1s/step - loss: 0.1523 - acc: 0.9440 - val_loss: 0.1316 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.13783 to 0.13163, saving model to weights_6_final_6_aug.08-0.13.hdf5\n",
      "Epoch 9/10\n",
      "820/820 [==============================] - 822s 1s/step - loss: 0.1525 - acc: 0.9439 - val_loss: 0.1357 - val_acc: 0.9492\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.13163\n",
      "Epoch 10/10\n",
      "820/820 [==============================] - 826s 1s/step - loss: 0.1512 - acc: 0.9435 - val_loss: 0.2454 - val_acc: 0.9082\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.13163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fc96a3be0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn6_final.fit_generator(train_generator_6_final, epochs=10, steps_per_epoch=train_steps_per_epoch_6_final, validation_data=val_generator_6_final, validation_steps=val_steps_per_epoch_6_final, callbacks=[\n",
    "                         csv_logger_6_final_6_aug, early_stop_6_final_6_aug, nan_problem_6_final_6_aug, reduce_lr_6_final_6_aug, saving_weights_6_final_6_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.save('cnn6_final_6_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model : weights_6_final_6_aug.08-0.13.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final = load_model('/home/ec2-user/SageMaker/models/cnn6_final_6_aug.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Epochs 61 - 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_weights_6_final_7_aug = keras.callbacks.ModelCheckpoint(\n",
    "    'weights_6_final_7_aug.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "    monitor='val_loss', verbose=2, save_best_only=True,\n",
    "    save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "reduce_lr_6_final_7_aug = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=20,\n",
    "    verbose=2, mode='auto', min_delta=0.0001, min_lr=0)\n",
    "\n",
    "nan_problem_6_final_7_aug = keras.callbacks.TerminateOnNaN()\n",
    "\n",
    "early_stop_6_final_7_aug = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=20,\n",
    "    verbose=2, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "csv_logger_6_final_7_aug = keras.callbacks.CSVLogger(\n",
    "    'training_6_final_7_aug.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "820/820 [==============================] - 853s 1s/step - loss: 0.1480 - acc: 0.9456 - val_loss: 0.1666 - val_acc: 0.9372\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16663, saving model to weights_6_final_7_aug.01-0.17.hdf5\n",
      "Epoch 2/10\n",
      "820/820 [==============================] - 827s 1s/step - loss: 0.1486 - acc: 0.9446 - val_loss: 0.1350 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16663 to 0.13495, saving model to weights_6_final_7_aug.02-0.13.hdf5\n",
      "Epoch 3/10\n",
      "820/820 [==============================] - 840s 1s/step - loss: 0.1467 - acc: 0.9451 - val_loss: 0.1296 - val_acc: 0.9515\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13495 to 0.12956, saving model to weights_6_final_7_aug.03-0.13.hdf5\n",
      "Epoch 4/10\n",
      "820/820 [==============================] - 826s 1s/step - loss: 0.1443 - acc: 0.9469 - val_loss: 0.1376 - val_acc: 0.9480\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12956\n",
      "Epoch 5/10\n",
      "820/820 [==============================] - 827s 1s/step - loss: 0.1433 - acc: 0.9475 - val_loss: 0.1633 - val_acc: 0.9397\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.12956\n",
      "Epoch 6/10\n",
      "820/820 [==============================] - 825s 1s/step - loss: 0.1445 - acc: 0.9468 - val_loss: 0.1268 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12956 to 0.12677, saving model to weights_6_final_7_aug.06-0.13.hdf5\n",
      "Epoch 7/10\n",
      "820/820 [==============================] - 827s 1s/step - loss: 0.1431 - acc: 0.9470 - val_loss: 0.1249 - val_acc: 0.9539\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12677 to 0.12489, saving model to weights_6_final_7_aug.07-0.12.hdf5\n",
      "Epoch 8/10\n",
      "820/820 [==============================] - 827s 1s/step - loss: 0.1414 - acc: 0.9482 - val_loss: 0.1369 - val_acc: 0.9498\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.12489\n",
      "Epoch 9/10\n",
      "820/820 [==============================] - 826s 1s/step - loss: 0.1407 - acc: 0.9485 - val_loss: 0.1283 - val_acc: 0.9534\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.12489\n",
      "Epoch 10/10\n",
      "820/820 [==============================] - 824s 1s/step - loss: 0.1368 - acc: 0.9499 - val_loss: 0.3014 - val_acc: 0.8918\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.12489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb3cea35940>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn6_final.fit_generator(train_generator_6_final, epochs=10, steps_per_epoch=train_steps_per_epoch_6_final, validation_data=val_generator_6_final, validation_steps=val_steps_per_epoch_6_final, callbacks=[\n",
    "                         csv_logger_6_final_7_aug, early_stop_6_final_7_aug, nan_problem_6_final_7_aug, reduce_lr_6_final_7_aug, saving_weights_6_final_7_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn6_final.save('cnn6_final_7_aug.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
